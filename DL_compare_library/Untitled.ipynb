{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n",
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우의 기본적인 구성을 익힙니다.\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.constant: 말 그대로 상수입니다.\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)  # a + b 로도 쓸 수 있음\n",
    "print(c)\n",
    "\n",
    "# 위에서 변수와 수식들을 정의했지만, 실행이 정의한 시점에서 실행되는 것은 아닙니다.\n",
    "# 다음처럼 Session 객제와 run 메소드를 사용할 때 계산이 됩니다.\n",
    "# 따라서 모델을 구성하는 것과, 실행하는 것을 분리하여 프로그램을 깔끔하게 작성할 수 있습니다.\n",
    "# 그래프를 실행할 세션을 구성합니다.\n",
    "sess = tf.Session()\n",
    "# sess.run: 설정한 텐서 그래프(변수나 수식 등등)를 실행합니다.\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "# 세션을 닫습니다.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[ 1.046084  -0.5918523]\n",
      " [ 0.5732441 -1.153867 ]\n",
      " [-1.1812537 -0.2627272]]\n",
      "=== b ===\n",
      "[[ 1.2947854]\n",
      " [-1.6485839]]\n",
      "=== expr ===\n",
      "[[ -0.05640352  -2.3929825 ]\n",
      " [ -1.6855493  -11.3616905 ]]\n"
     ]
    }
   ],
   "source": [
    "# 플레이스홀더와 변수의 개념을 익혀봅니다\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.placeholder: 계산을 실행할 때 입력값을 받는 변수로 사용합니다.\n",
    "# None 은 크기가 정해지지 않았음을 의미합니다.\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)\n",
    "\n",
    "# X 플레이스홀더에 넣을 값 입니다.\n",
    "# 플레이스홀더에서 설정한 것 처럼, 두번째 차원의 요소의 갯수는 3개 입니다.\n",
    "x_data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "# tf.Variable: 그래프를 계산하면서 최적화 할 변수들입니다. 이 값이 바로 신경망을 좌우하는 값들입니다.\n",
    "# tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화합니다.\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "# 입력값과 변수들을 계산할 수식을 작성합니다.\n",
    "# tf.matmul 처럼 mat* 로 되어 있는 함수로 행렬 계산을 수행합니다.\n",
    "expr = tf.matmul(X, W) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "# 위에서 설정한 Variable 들의 값들을 초기화 하기 위해\n",
    "# 처음에 tf.global_variables_initializer 를 한 번 실행해야 합니다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# expr 수식에는 X 라는 입력값이 필요합니다.\n",
    "# 따라서 expr 실행시에는 이 변수에 대한 실제 입력값을 다음처럼 넣어줘야합니다.\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", dtype=float32)\n",
      "Tensor(\"Y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행해봅니다.\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 14.028338 [0.84745526] [0.77727604]\n",
      "1 0.23847342 [0.6789199] [0.68283874]\n",
      "2 0.070383035 [0.7054592] [0.674703]\n",
      "3 0.06516723 [0.7104828] [0.6575787]\n",
      "4 0.06204937 [0.7176674] [0.64186984]\n",
      "5 0.059101697 [0.7244299] [0.6264289]\n",
      "6 0.05629431 [0.7310571] [0.61137116]\n",
      "7 0.053620264 [0.737522] [0.5966741]\n",
      "8 0.051073316 [0.7438319] [0.58233047]\n",
      "9 0.048647266 [0.7499899] [0.5683316]\n",
      "10 0.04633649 [0.75600004] [0.5546693]\n",
      "11 0.04413547 [0.76186556] [0.5413354]\n",
      "12 0.042039007 [0.7675902] [0.5283221]\n",
      "13 0.04004209 [0.77317715] [0.5156216]\n",
      "14 0.03814009 [0.7786298] [0.5032264]\n",
      "15 0.03632841 [0.78395146] [0.49112922]\n",
      "16 0.034602787 [0.7891451] [0.4793228]\n",
      "17 0.03295909 [0.79421383] [0.46780017]\n",
      "18 0.03139354 [0.79916084] [0.4565546]\n",
      "19 0.029902315 [0.8039889] [0.44557935]\n",
      "20 0.028481951 [0.80870086] [0.43486792]\n",
      "21 0.027129024 [0.81329954] [0.42441398]\n",
      "22 0.025840396 [0.81778777] [0.4142114]\n",
      "23 0.024612939 [0.82216793] [0.404254]\n",
      "24 0.023443809 [0.8264429] [0.39453602]\n",
      "25 0.022330225 [0.8306151] [0.38505164]\n",
      "26 0.021269495 [0.834687] [0.37579525]\n",
      "27 0.0202592 [0.838661] [0.3667614]\n",
      "28 0.01929685 [0.8425395] [0.3579447]\n",
      "29 0.018380241 [0.8463248] [0.34933996]\n",
      "30 0.017507177 [0.85001904] [0.34094205]\n",
      "31 0.016675556 [0.8536244] [0.33274603]\n",
      "32 0.01588345 [0.8571432] [0.32474706]\n",
      "33 0.015128974 [0.8605774] [0.31694037]\n",
      "34 0.0144103505 [0.86392903] [0.3093213]\n",
      "35 0.013725837 [0.8672001] [0.30188543]\n",
      "36 0.013073866 [0.8703925] [0.29462832]\n",
      "37 0.012452838 [0.87350816] [0.28754565]\n",
      "38 0.011861317 [0.87654895] [0.28063327]\n",
      "39 0.011297909 [0.87951666] [0.27388704]\n",
      "40 0.01076125 [0.88241297] [0.26730296]\n",
      "41 0.010250063 [0.88523966] [0.2608772]\n",
      "42 0.009763189 [0.8879984] [0.2546059]\n",
      "43 0.009299439 [0.89069086] [0.24848536]\n",
      "44 0.008857704 [0.89331853] [0.24251193]\n",
      "45 0.008436953 [0.89588314] [0.23668213]\n",
      "46 0.008036184 [0.898386] [0.23099245]\n",
      "47 0.0076544634 [0.9008287] [0.22543955]\n",
      "48 0.0072908816 [0.9032128] [0.22002016]\n",
      "49 0.006944558 [0.9055395] [0.21473102]\n",
      "50 0.006614679 [0.9078103] [0.20956902]\n",
      "51 0.0063004796 [0.9100264] [0.2045311]\n",
      "52 0.006001197 [0.9121893] [0.19961433]\n",
      "53 0.0057161413 [0.9143002] [0.19481574]\n",
      "54 0.005444623 [0.91636044] [0.19013253]\n",
      "55 0.005185991 [0.91837096] [0.18556184]\n",
      "56 0.0049396604 [0.9203333] [0.18110107]\n",
      "57 0.004705017 [0.9222485] [0.17674753]\n",
      "58 0.004481524 [0.9241175] [0.17249861]\n",
      "59 0.0042686523 [0.9259417] [0.16835189]\n",
      "60 0.004065888 [0.92772204] [0.16430482]\n",
      "61 0.0038727543 [0.9294596] [0.16035505]\n",
      "62 0.0036887974 [0.93115526] [0.1565002]\n",
      "63 0.0035135746 [0.9328103] [0.15273806]\n",
      "64 0.0033466788 [0.9344255] [0.14906633]\n",
      "65 0.003187699 [0.9360018] [0.14548287]\n",
      "66 0.0030362934 [0.93754035] [0.1419856]\n",
      "67 0.0028920595 [0.9390418] [0.13857234]\n",
      "68 0.0027546908 [0.94050723] [0.13524117]\n",
      "69 0.0026238358 [0.9419373] [0.13199003]\n",
      "70 0.0024992109 [0.9433332] [0.12881711]\n",
      "71 0.0023804915 [0.94469535] [0.1257204]\n",
      "72 0.0022674154 [0.94602484] [0.12269817]\n",
      "73 0.0021597135 [0.9473224] [0.11974861]\n",
      "74 0.0020571246 [0.94858867] [0.11686991]\n",
      "75 0.0019594037 [0.94982463] [0.11406047]\n",
      "76 0.0018663355 [0.9510308] [0.11131852]\n",
      "77 0.0017776797 [0.952208] [0.1086425]\n",
      "78 0.0016932414 [0.95335686] [0.10603081]\n",
      "79 0.0016128105 [0.95447814] [0.1034819]\n",
      "80 0.0015362022 [0.9555724] [0.10099426]\n",
      "81 0.0014632285 [0.9566404] [0.09856643]\n",
      "82 0.0013937271 [0.9576828] [0.09619697]\n",
      "83 0.0013275258 [0.95870006] [0.09388447]\n",
      "84 0.0012644677 [0.9596929] [0.09162756]\n",
      "85 0.0012044015 [0.9606618] [0.0894249]\n",
      "86 0.0011471956 [0.9616075] [0.08727519]\n",
      "87 0.0010926977 [0.9625304] [0.08517714]\n",
      "88 0.0010407971 [0.9634312] [0.08312955]\n",
      "89 0.0009913555 [0.9643102] [0.08113116]\n",
      "90 0.00094426476 [0.9651682] [0.07918083]\n",
      "91 0.00089941546 [0.96600556] [0.07727741]\n",
      "92 0.0008566917 [0.96682274] [0.07541969]\n",
      "93 0.0008159953 [0.9676203] [0.07360666]\n",
      "94 0.00077723706 [0.9683987] [0.0718372]\n",
      "95 0.00074031577 [0.96915835] [0.07011028]\n",
      "96 0.0007051516 [0.9698998] [0.0684249]\n",
      "97 0.00067165616 [0.9706233] [0.06677999]\n",
      "98 0.00063975167 [0.9713295] [0.06517465]\n",
      "99 0.00060936454 [0.9720188] [0.06360791]\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [4.923702]\n",
      "X: 2.5, Y: [2.493655]\n"
     ]
    }
   ],
   "source": [
    "# X 와 Y 의 상관 관계를 분석하기 위한 가설 수식을 작성합니다.\n",
    "# y = W * x + b\n",
    "# W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용했습니다.\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# 손실 함수를 작성합니다.\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정합니다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# 비용을 최소화 하는 것이 최종 목표\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 세션을 생성하고 초기화합니다.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행합니다.\n",
    "    for step in range(100):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.1074581\n",
      "20 1.1061606\n",
      "30 1.1048726\n",
      "40 1.1036302\n",
      "50 1.1029425\n",
      "60 1.1022652\n",
      "70 1.1015979\n",
      "80 1.1009408\n",
      "90 1.1002934\n",
      "100 1.0996556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 2 0 0 0 0]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 50.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax 를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.73850054\n",
      "20 0.57052016\n",
      "30 0.45567143\n",
      "40 0.3669914\n",
      "50 0.29274943\n",
      "60 0.23246862\n",
      "70 0.18435293\n",
      "80 0.14633636\n",
      "90 0.116367035\n",
      "100 0.09299555\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다.\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
